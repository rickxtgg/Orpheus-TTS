# Orpheus-TTS 项目技术说明文档

## 1. 项目概述

Orpheus-TTS 是一个基于 Llama-3b 骨干网络构建的开源文本转语音（Text-to-Speech，TTS）系统。该项目展示了大型语言模型（LLM）在语音合成领域的新兴能力，其性能超越了一些领先的闭源模型，如 Eleven Labs 和 PlayHT。

### 1.1 核心功能

- **类人语音**：自然的语调、情感和节奏，优于当前最先进的闭源模型
- **零样本声音克隆**：无需事先微调即可克隆声音
- **引导式情感和语调控制**：通过简单的标签控制语音和情感特征
- **低延迟**：实时应用的流式传输延迟约为 200ms，使用输入流可降至约 100ms

## 2. 项目结构

```
/
├── .gitignore
├── LICENSE
├── README.md
├── demo.mp4
├── emotions.txt                    # 支持的情感标签列表
├── finetune/                       # 微调模型相关文件
│   ├── config.yaml                 # 微调配置文件
│   └── train.py                    # 微调训练脚本
├── pretrain/                       # 预训练模型相关文件
│   ├── config.yaml                 # 预训练配置文件
│   ├── readme.md                   # 预训练说明文档
│   └── train.py                    # 预训练训练脚本
└── realtime_streaming_example/     # 实时流式传输示例
    ├── client.html                 # 客户端示例
    └── main.py                     # 服务器端示例
```

## 3. 模型架构

Orpheus-TTS 提供了两种主要模型：

1. **预训练模型（Pretrained）**：基础模型，在超过 10 万小时的英语语音数据上训练
   - 模型地址：[canopylabs/orpheus-tts-0.1-pretrained](https://huggingface.co/canopylabs/orpheus-tts-0.1-pretrained)
   - 基于 Llama-3.2-3B-Instruct 模型构建

2. **微调生产模型（Finetuned Prod）**：针对日常 TTS 应用的微调模型
   - 模型地址：[canopylabs/orpheus-tts-0.1-finetune-prod](https://huggingface.co/canopylabs/orpheus-tts-0.1-finetune-prod)
   - 基于预训练模型进一步优化

## 4. 训练流程

### 4.1 预训练流程

预训练过程使用了两个数据集：文本问答数据集和 TTS 数据集，通过比例混合的方式进行训练。

预训练的主要特点：

- 使用 FSDP（Fully Sharded Data Parallel）进行分布式训练
- 实现了自定义的 `BatchedRatioDataset` 类，按照配置的比例混合两个数据集
- 使用 `AlternatingDistributedSampler` 进行数据采样
- 添加了大量自定义 token（7 * 4096 + 10 个）
- 使用 flash attention 2 优化注意力机制
- 使用余弦学习率调度器

预训练过程使用了两个数据集：文本问答数据集和 TTS 数据集，通过比例混合的方式进行训练。以下是详细的预训练步骤和代码示例：

#### 4.1.1 数据准备

1. **数据集格式**
   ```python
   # 数据集示例格式
   # 每个样本包含以下字段：
   # - text: 需要转换为语音的文本内容
   # - audio: 对应的音频文件路径，支持wav格式
   # - speaker_id: 说话者的唯一标识符，用于区分不同说话者
   # - emotion: 可选字段，用于指定情感标签，如happy、sad等
   {
       "text": "输入文本",
       "audio": "音频文件路径",
       "speaker_id": "说话者ID",
       "emotion": "情感标签"  # 可选
   }
   ```

2. **自定义数据集类**
   ```python
   # BatchedRatioDataset类用于混合文本问答数据集和TTS数据集
   # - text_dataset: 文本问答数据集，用于增强模型的语言理解能力
   # - tts_dataset: TTS数据集，包含文本和对应的音频数据
   # - ratio: 混合比例，表示从text_dataset中采样的概率
   class BatchedRatioDataset(Dataset):
       def __init__(self, text_dataset, tts_dataset, ratio=0.5):
           # 初始化两个数据集和混合比例
           self.text_dataset = text_dataset  # 文本问答数据集
           self.tts_dataset = tts_dataset    # TTS数据集
           self.ratio = ratio                # 混合比例，默认0.5
           # 数据集长度取两个数据集中的较大值
           self.length = max(len(text_dataset), len(tts_dataset))

       def __len__(self):
           return self.length

       # 根据ratio概率随机返回text_dataset或tts_dataset中的样本
       # 使用取模运算确保索引不会越界
       def __getitem__(self, idx):
           if random.random() < self.ratio:
               return self.text_dataset[idx % len(self.text_dataset)]
           return self.tts_dataset[idx % len(self.tts_dataset)]
   ```

3. **自定义采样器**
   ```python
   # AlternatingDistributedSampler用于分布式训练中的数据采样
   # 继承自PyTorch的DistributedSampler，实现了交替采样策略
   class AlternatingDistributedSampler(DistributedSampler):
       def __init__(self, dataset, num_replicas=None, rank=None, shuffle=True):
           # num_replicas: 分布式训练的总进程数
           # rank: 当前进程的rank
           # shuffle: 是否打乱数据顺序
           super().__init__(dataset, num_replicas=num_replicas, rank=rank, shuffle=shuffle)

       def __iter__(self):
           # 生成数据集索引列表
           indices = list(range(len(self.dataset)))
           if self.shuffle:
               # 使用epoch作为随机种子，确保每个epoch的打乱顺序不同
               random.seed(self.epoch)
               random.shuffle(indices)
           # 返回当前进程负责的数据索引
           # self.rank:self.total_size:self.num_replicas 表示数据分片
           return iter(indices[self.rank:self.total_size:self.num_replicas])
   ```

#### 4.1.2 模型配置

1. **FSDP 配置**
   ```python
   # 导入FSDP相关组件
   from torch.distributed.fsdp import (
       FullyShardedDataParallel as FSDP,  # 完全分片数据并行
       MixedPrecision,                    # 混合精度训练
       BackwardPrefetch,                  # 反向传播预取
   )

   # FSDP配置字典
   fsdp_config = {
       # 混合精度配置：使用float16进行计算以提高训练效率和减少显存使用
       "mixed_precision": MixedPrecision(
           param_dtype=torch.float16,    # 模型参数使用float16
           reduce_dtype=torch.float16,   # 梯度聚合使用float16
           buffer_dtype=torch.float16,   # 模型buffer使用float16
       ),
       # 启用反向传播预取，提高训练效率
       "backward_prefetch": BackwardPrefetch.BACKWARD_PRE,
       # 启用激活检查点，用于节省显存
       "activation_checkpointing": True,
   }
   ```

2. **优化器和学习率配置**
   ```python
   def get_optimizer_and_scheduler(model, config):
       # 创建AdamW优化器
       # AdamW是Adam的变体，提供了更好的权重衰减实现
       optimizer = torch.optim.AdamW(
           model.parameters(),           # 模型参数
           lr=config.learning_rate,      # 初始学习率
           weight_decay=0.01,            # L2正则化系数
           betas=(0.9, 0.999),          # 动量参数
           eps=1e-8                      # 数值稳定性参数
       )

       # 创建带有预热的余弦学习率调度器
       # 学习率先线性增加，然后按余弦曲线衰减
       scheduler = get_cosine_schedule_with_warmup(
           optimizer,
           num_warmup_steps=config.warmup_steps,    # 预热步数
           num_training_steps=config.max_steps      # 总训练步数
       )

       return optimizer, scheduler
   ```

#### 4.1.3 训练流程

1. **主训练循环**
   ```python
   def train(config):
       # 加载预训练模型
       # 使用float16数据类型和flash attention 2来提高训练效率
       model = AutoModelForCausalLM.from_pretrained(
           config.model_name,              # 预训练模型名称
           torch_dtype=torch.float16,      # 使用float16精度
           use_flash_attention_2=True      # 启用flash attention 2
       )

       # 将模型包装为FSDP模型以进行分布式训练
       model = FSDP(model, **fsdp_config)
       # 获取优化器和学习率调度器
       optimizer, scheduler = get_optimizer_and_scheduler(model, config)

       # 训练循环
       for epoch in range(config.epochs):
           for batch in train_dataloader:
               # 前向传播
               outputs = model(**batch)
               loss = outputs.loss
               # 反向传播
               loss.backward()
               # 更新模型参数
               optimizer.step()
               # 更新学习率
               scheduler.step()
               # 清空梯度
               optimizer.zero_grad()

           # 定期保存检查点
           if epoch % config.save_epochs == 0:
               save_checkpoint(model, optimizer, scheduler, epoch)
   ```

2. **推荐训练参数**
   ```yaml
   # 预训练推荐参数
   batch_size: 32
   gradient_accumulation_steps: 4
   learning_rate: 5e-5
   warmup_steps: 1000
   max_steps: 100000
   save_epochs: 1
   fp16: true
   flash_attention: true
   dataset_ratio: 0.7  # TTS数据集比例
   ```

### 4.2 微调流程

微调过程主要针对特定声音或应用场景进行优化，通过在预训练模型基础上进行进一步训练来适应特定需求。以下是详细的微调步骤和注意事项：

#### 4.2.1 微调特点

- 使用 Hugging Face 的 Trainer API 进行训练
- 支持 flash attention 2 加速训练
- 使用 wandb 进行训练监控和可视化
- 建议至少使用 50 个样本，最佳效果需要约 300 个样本/说话者
- 支持多说话者和情感控制

#### 4.2.2 数据准备

1. **数据集格式要求**
   ```python
   # 微调数据集格式
   # 每个样本需要包含以下字段：
   # - text: 英文文本内容，用于生成语音
   # - audio_path: 对应的音频文件路径，必须是wav格式
   # - speaker_name: 说话者名称，用于区分不同说话者的声音特征
   # - emotion: 可选的情感标签，用于控制生成语音的情感色彩
   {
       "text": "Hello, this is a sample text",  # 英文文本
       "audio_path": "path/to/audio.wav",      # 音频文件路径
       "speaker_name": "speaker1",             # 说话者名称
       "emotion": "happy"                      # 情感标签（可选）
   }
   ```

2. **数据集处理**
   ```python
   def prepare_dataset(data_path):
       # 加载JSON格式的数据集
       # data_path应该指向包含所有样本的JSON文件
       dataset = load_dataset("json", data_files=data_path)
       
       # 加载与预训练模型匹配的分词器
       tokenizer = AutoTokenizer.from_pretrained(config.model_name)

       # 定义文本分词函数
       def tokenize_function(examples):
           # 将文本转换为模型输入格式
           return tokenizer(
               examples["text"],              # 输入文本
               padding="max_length",         # 使用最大长度填充
               truncation=True,             # 截断过长的文本
               max_length=512               # 最大序列长度
           )

       # 对数据集应用分词函数
       # batched=True可以加速处理
       return dataset.map(tokenize_function, batched=True)
   ```

#### 4.2.3 模型配置

1. **训练配置**
   ```python
   # 定义训练参数
   training_args = TrainingArguments(
       output_dir="./results",          # 输出目录
       num_train_epochs=3,             # 训练轮数
       per_device_train_batch_size=8,  # 每个设备的批次大小
       learning_rate=2e-5,             # 学习率
       warmup_steps=500,               # 预热步数
       weight_decay=0.01,              # 权重衰减
       logging_dir="./logs",           # 日志目录
       logging_steps=10,               # 日志记录间隔
       save_strategy="epoch",          # 保存策略
       fp16=True,                      # 使用混合精度训练
       gradient_accumulation_steps=4,  # 梯度累积步数
   )
   ```

2. **模型初始化**
   ```python
   def init_model(config):
       # 加载预训练模型
       model = AutoModelForCausalLM.from_pretrained(
           config.model_name,
           torch_dtype=torch.float16,      # 使用float16精度
           use_flash_attention_2=True      # 启用flash attention 2
       )
       
       # 冻结部分层以加快训练
       for param in model.base_model.parameters():
           param.requires_grad = False
       
       return model
   ```

#### 4.2.4 训练流程

1. **训练循环**
   ```python
   def finetune(config, model, dataset):
       # 初始化训练器
       trainer = Trainer(
           model=model,                    # 模型
           args=training_args,             # 训练参数
           train_dataset=dataset["train"], # 训练数据集
           tokenizer=tokenizer,            # 分词器
           data_collator=data_collator,    # 数据整理器
       )
       
       # 开始训练
       trainer.train()
       
       # 保存模型
       trainer.save_model()
   ```

2. **训练监控**
   ```python
   # 初始化wandb
   wandb.init(
       project="orpheus-tts-finetune",
       config={
           "learning_rate": training_args.learning_rate,
           "epochs": training_args.num_train_epochs,
           "batch_size": training_args.per_device_train_batch_size
       }
   )
   
   # 添加自定义回调
   class WandbCallback(TrainerCallback):
       def on_log(self, args, state, control, logs=None, **kwargs):
           if logs:
               wandb.log(logs)
   
   trainer.add_callback(WandbCallback())
   ```

#### 4.2.5 注意事项

1. **数据质量控制**
   - 确保音频文件质量良好，无背景噪音
   - 文本内容应准确对应音频内容
   - 音频长度建议在5-15秒之间

2. **训练建议**
   - 从小批量数据开始测试
   - 定期验证生成的语音质量
   - 适时调整学习率和训练轮数
   - 保存训练过程中的最佳模型

3. **资源要求**
   - 建议使用具有至少16GB显存的GPU
   - 预留足够的磁盘空间（约50GB）
   - 推荐使用SSD以提高数据加载速度

微调过程针对特定声音或应用场景进行优化，以下是详细步骤和代码示例：

#### 4.2.1 数据集准备

1. **数据集格式要求**
   ```python
   # 微调数据集格式
   # 每个样本需要包含以下字段：
   # - text: 英文文本内容，用于生成语音
   # - audio_path: 对应的音频文件路径，必须是wav格式
   # - speaker_name: 说话者名称，用于区分不同说话者的声音特征
   # - emotion: 可选的情感标签，用于控制生成语音的情感色彩
   {
       "text": "Hello, this is a sample text",  # 英文文本
       "audio_path": "path/to/audio.wav",      # 音频文件路径
       "speaker_name": "speaker1",             # 说话者名称
       "emotion": "happy"                      # 情感标签（可选）
   }
   ```

2. **数据集处理**
   ```python
   def prepare_dataset(data_path):
       # 加载JSON格式的数据集
       # data_path应该指向包含所有样本的JSON文件
       dataset = load_dataset("json", data_files=data_path)
       
       # 加载与预训练模型匹配的分词器
       tokenizer = AutoTokenizer.from_pretrained(config.model_name)

       # 定义文本分词函数
       def tokenize_function(examples):
           # 将文本转换为模型输入格式
           return tokenizer(
               examples["text"],              # 输入文本
               padding="max_length",         # 使用最大长度填充
               truncation=True,             # 截断过长的文本
               max_length=512               # 最大序列长度
           )

       # 对数据集应用分词函数
       # batched=True可以加速处

#### 4.2.2 训练配置

1. **Trainer 配置**
   ```python
   training_args = TrainingArguments(
       output_dir="./results",
       learning_rate=2e-5,
       num_train_epochs=3,
       per_device_train_batch_size=4,
       gradient_accumulation_steps=4,
       warmup_steps=500,
       weight_decay=0.01,
       logging_dir="./logs",
       logging_steps=10,
       save_steps=1000,
       fp16=True,
       report_to="wandb"
   )
   ```

2. **模型训练**
   ```python
   def train_model():
       model = AutoModelForCausalLM.from_pretrained(
           config.model_name,
           torch_dtype=torch.float16,
           use_flash_attention_2=True
       )

       trainer = Trainer(
           model=model,
           args=training_args,
           train_dataset=train_dataset,
           data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
       )

       trainer.train()
   ```

#### 4.2.3 最佳实践参数

```yaml
# 微调推荐参数
learning_rate: 2e-5
num_train_epochs: 3
batch_size: 4
gradient_accumulation_steps: 4
warmup_steps: 500
weight_decay: 0.01
max_grad_norm: 1.0
fp16: true
flash_attention: true

# 数据集建议
min_samples: 50        # 最小样本数
optimal_samples: 300   # 最佳样本数/说话者
max_text_length: 512   # 最大文本长度
audio_sample_rate: 24000  # 音频采样率
```

#### 4.2.4 训练监控

1. **WandB 配置**
   ```python
   import wandb

   wandb.init(
       project="orpheus-tts-finetune",
       config={
           "learning_rate": training_args.learning_rate,
           "epochs": training_args.num_train_epochs,
           "batch_size": training_args.per_device_train_batch_size,
           "model_name": config.model_name,
       }
   )
   ```

2. **自定义回调**
   ```python
   class CustomCallback(TrainerCallback):
       def on_log(self, args, state, control, logs=None, **kwargs):
           if state.is_world_process_zero:
               wandb.log(logs)

       def on_evaluate(self, args, state, control, metrics=None, **kwargs):
           if state.is_world_process_zero:
               wandb.log(metrics)
   ```

## 5. 配置参数说明

### 5.1 预训练配置（pretrain/config.yaml）

```yaml
# 模型
model_name: "meta-llama/Llama-3.2-3B-Instruct"  # 基础模型
tokenizer_name: "meta-llama/Llama-3.2-3B-Instruct"

# 训练参数
epochs: 1                # 训练轮数
batch_size: 1            # 每个设备的批次大小
number_processes: 8      # 进程数
pad_token: 128263        # 填充标记
save_steps: 12000        # 保存检查点的步数
learning_rate: 5.0e-5    # 学习率
ratio: <see read me>     # 数据集混合比例

# 数据集
text_QA_dataset: <speech input-ids>  # 文本问答数据集
TTS_dataset: <text-input-ids>        # TTS 数据集

# 命名和路径
save_folder: "checkpoints"           # 保存检查点的文件夹
project_name: "pretrain-orpheus"    # wandb 项目名称
run_name: "pretrain-orpheus"        # wandb 运行名称
```

### 5.2 微调配置（finetune/config.yaml）

```yaml
# 数据集
TTS_dataset: <PATH_TO_YOUR_DATASET>  # 您的数据集路径

model_name: "canopylabs/orpheus-tts-0.1-pretrained"  # 预训练模型

# 训练参数
epochs: 1                # 训练轮数
batch_size: 1            # 批次大小
number_processes: 1      # 进程数
pad_token: 128263        # 填充标记
save_steps: 5000         # 保存检查点的步数
learning_rate: 5.0e-5    # 学习率

# 命名和路径
save_folder: "checkpoints"  # 保存检查点的文件夹
project_name: "tuning-orpheus"  # wandb 项目名称
run_name: "5e5-0"            # wandb 运行名称
```

## 6. 使用指南

### 6.1 简单推理设置

可以使用 Google Colab 快速开始：

1. [微调模型的 Colab](https://colab.research.google.com/drive/1KhXT56UePPUHhqitJNUxq63k-pQomz3N?usp=sharing)
2. [预训练模型的 Colab](https://colab.research.google.com/drive/10v9MIEbZOr_3V8ZcPAIh8MN7q2LjcstS?usp=sharing)

### 6.2 流式推理示例

1. 克隆仓库并安装依赖：
   ```bash
   git clone https://github.com/canopyai/Orpheus-TTS.git
   cd Orpheus-TTS
   pip install orpheus-speech  # 底层使用 vllm 进行快速推理
   ```

2. 如果遇到 vllm 版本问题，可以回退到稳定版本：
   ```bash
   pip install vllm==0.7.3
   ```

3. 基本使用示例：
   ```python
   from orpheus_tts import OrpheusModel
   import wave
   import time
   
   model = OrpheusModel(model_name="canopylabs/orpheus-tts-0.1-finetune-prod")
   prompt = '''Man, the way social media has, um, completely changed how we interact is just wild, right? Like, we're all connected 24/7 but somehow people feel more alone than ever. And don't even get me started on how it's messing with kids' self-esteem and mental health and whatnot.'''

   start_time = time.monotonic()
   syn_tokens = model.generate_speech(
      prompt=prompt,
      voice="tara",
      )

   with wave.open("output.wav", "wb") as wf:
      wf.setnchannels(1)
      wf.setsampwidth(2)
      wf.setframerate(24000)

      total_frames = 0
      chunk_counter = 0
      for audio_chunk in syn_tokens:  # 输出流
         chunk_counter += 1
         frame_count = len(audio_chunk) // (wf.getsampwidth() * wf.getnchannels())
         total_frames += frame_count
         wf.writeframes(audio_chunk)
      duration = total_frames / wf.getframerate()

      end_time = time.monotonic()
      print(f"It took {end_time - start_time} seconds to generate {duration:.2f} seconds of audio")
   ```

### 6.3 实时流式服务器示例

项目提供了一个基于 Flask 的实时流式服务器示例：

```python
# realtime_streaming_example/main.py
from flask import Flask, Response, request
import struct
from orpheus_tts import OrpheusModel

app = Flask(__name__)
engine = OrpheusModel(model_name="canopylabs/orpheus-tts-0.1-finetune-prod")

@app.route('/tts', methods=['GET'])
def tts():
    prompt = request.args.get('prompt', 'Hey there, looks like you forgot to provide a prompt!')

    def generate_audio_stream():
        yield create_wav_header()

        syn_tokens = engine.generate_speech(
            prompt=prompt,
            voice="tara",
            repetition_penalty=1.1,
            stop_token_ids=[128258],
            max_tokens=2000,
            temperature=0.4,
            top_p=0.9
        )
        for chunk in syn_tokens:
            yield chunk

    return Response(generate_audio_stream(), mimetype='audio/wav')

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080, threaded=True)
```

### 6.4 提示词格式

1. **微调生产模型**：文本提示格式为 `{name}: I went to the ...`
   - 可用的名称（按对话真实性排序）："tara", "leah", "jess", "leo", "dan", "mia", "zac", "zoe"
   - 可添加情感标签：`<laugh>`, `<chuckle>`, `<sigh>`, `<cough>`, `<sniffle>`, `<groan>`, `<yawn>`, `<gasp>`

2. **预训练模型**：可以仅基于文本生成语音，或在提示中包含一个或多个现有的文本-语音对

3. **其他情感标签**：项目支持多种情感标签，包括：
   ```
   happy, normal, disgust, longer, sad, frustrated, slow, excited, whisper, panicky, curious, surprise, fast, crying, deep, sleepy, angry, high, shout
   ```

### 6.5 微调自己的模型

1. 准备数据集：数据集应为 Hugging Face 格式，参考[示例数据集](https://huggingface.co/datasets/canopylabs/zac-sample-dataset)

2. 使用[此 Notebook](https://colab.research.google.com/drive/1wg_CPCA-MzsWtsujwy-1Ovhv-tn8Q1nD?usp=sharing)准备数据，处理速度约为每千行不到 1 分钟

3. 修改 `finetune/config.yaml` 文件，包含您的数据集和训练属性，然后运行训练脚本

4. 安装必要的依赖：
   ```bash
   pip install transformers datasets wandb trl flash_attn torch
   huggingface-cli login <输入您的 HF 令牌>
   ```

## 7. 性能与限制

- 流式传输延迟：约 200ms，可降至约 100ms
- 语音质量：超越当前最先进的闭源模型
- 微调效果：50 个样本开始见效，最佳效果需要约 300 个样本/说话者
- 资源需求：预训练需要较大的计算资源，微调可在单 GPU 上进行

## 8. 总结

Orpheus-TTS 是一个强大的开源文本转语音系统，基于 Llama-3b 骨干网络构建。它提供了类人语音、零样本声音克隆、情感控制和低延迟等功能，性能超越了一些领先的闭源模型。项目包含预训练和微调两个主要组件，并提供了详细的使用指南和示例代码，使用户能够轻松地使用现有模型或训练自己的模型。